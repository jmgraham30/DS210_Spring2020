---
title: "Introduction to Probability and Statistics"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

```{r,warning=FALSE,message=FALSE,echo=FALSE}
library(tidyverse)
library(fastR2)
library(knitr)
```


**Question:** If you flip a quarter, what are the chances that the coin lands with the "heads" side facing up? 

![ ](quarter.jpg){width=50%}

# Introduction

In this notebook, we will use a combination of logic, experiment, and computing to get an overview of (almost) all of the concepts from probability and statistics that will we cover in more detail as the course progresses. That is, we are going to get a glimpse of the main ideas of both probability and statistics (statistical inference). 

Data science and machine learning rely heavily on both probability and statistics. This [article](https://towardsdatascience.com/machine-learning-probability-statistics-f830f8c09326) provides some perspective on probability and statistics for machine learning. In addition, the following articles from Wikipedia are worth skimming:

* [Probability](https://en.wikipedia.org/wiki/Probability) is the measure of the likelihood that an event will occur.

* [Statistics](https://en.wikipedia.org/wiki/Statistics) is a branch of mathematics dealing with data collection, organization, analysis, interpretation and presentation.

* [Statistical inference](https://en.wikipedia.org/wiki/Statistical_inference) is the process of using data analysis to deduce properties of an underlying probability distribution.

One of the main takeaways that I would like for you to get from this introduction is that statistics (statistical inference) relies on probability. Thus, it is not really possible to understand statistics without probabilty. 

# Coin Flipping and a Probability Model 

Consider the flip of a *fair*, two-sided coin. Flipping the coin is an example of an (random) **experiment**. 

Suppose that we call one side of the coin heads and the other tails. After flipping the coin, the (only) possible **outcomes** are heads ($H$) or tails ($T$). 

In this case, we say that the "**sample space**" (set of all possible outcomes) for the coin flipping experiment is $\Omega=\{H,T\}$. For reasons that you will see later, we call the subsets of the sample space $\Omega$ the **events** of the experiment. Thus the events corresponding to the coin flipping sample space $\Omega$ are: $\emptyset$, $\{H\}$, $\{T\}$, and $\{H,T\}=\Omega$. (Recall that a set with $n$ elements has $2^{n}$ possible subsets.) 

We call each run of an experiment (*e.g.*, flip a coin) a **trial**.  

By a fair coin, we mean that the chances of the coin landing with heads facing up when flipped is the same as it landing with tails facing up. Thus, it is commonly said that the coin has a 50\% chance of turning up heads (and also a 50\% chance of turning up tails). 

Another way to say this is that the probability that the coin lands heads up is 1/2 (so the probability of landing a tails is also 1/2). 

In notation, 

$P(\{H\}) = \frac{1}{2}$ 

and 

$P(\{T\})=\frac{1}{2}$.

(For technical reasons we also write that $P(\emptyset)=0$, and $P(\Omega)=1$). Note that we will always take probability values to be nonnegative numbers less than or equal to 1. 

Our goal is to construct a robust mathematical model (*i.e.*, a probability model) for the coin tossing experiment (and more generally any random experiment). One initial annoyance is that our outcomes are described by the elements of the set $\Omega$  and these are not numerical quantities which are much easier to work with in practice (especially on a computer). There is a very powerful idea that will allow us to convert general questions about probabilities into statments about numberical quantities. 


For example, we can convert the coin flipping experiment into a mathematical function $X:\Omega \rightarrow \{0,1\}$ that takes on numerical values by defining 

$X(\{H\}) = 1$, 

and 

$X(\{T\}) = 0$. 

That is, we are simply counting the number of heads after one flip of a coin. A function like this is called a **random variable**. Since we are now working with numerical values, we can exploit this to develop a mathematical model for coin flipping, because the probability that the random variable $X$ will equal 1 is $\frac{1}{2}$ and the probability that $X$ will equal 0 is also $\frac{1}{2}$. That is, 

$P(X=1) = \frac{1}{2}$, and $P(X=0) = \frac{1}{2}$


We can go even further and write an expression for this, if $x\in \{0,1\}$, 

$p(x) = P(X=x) = \left(\frac{1}{2}\right)^{x}\left(1 - \frac{1}{2}\right)^{1 - x}$

A function like $p$ is called a **probability mass function** (pmf). There is also a realted function called the **cummulative distribution function** (CDF) denoted $F(x)$ and defined as

$F(x) = \sum_{s \leq x}p(s)$ for $x\in \{0,1\}$. 

Thus,

$F(0) = p(0) = \frac{1}{2}$, 

and 

$F(1) = p(0) + p(1) = \frac{1}{2} + \frac{1}{2} = 1$. 


These are implemented in R by the functions dbinom and pbinom respectively. For example,
```{r}
dbinom(1,size=1,prob=0.5)
```
and
```{r}
pbinom(1,size=1,prob=0.5)
```

**Exercise** Try varying the first number in the input to the functions in the last two lines of code. 

In addition, we can visualize these functions in R.
```{r}
gf_dist("binom",params=list(size=1,prob=0.5),kind="density")
```

```{r}
gf_dist("binom",params=list(size=1,prob=0.5),kind="cdf")
```


The benefit of all of this is that now it is easy to abstract to the situation where we want to compute the probability of getting a certain number $x$ of heads after flipping the coin $n$ times, that is, after $n$ trials. 

**Exercise** What would be an appropriate sample space for the experiment of flipping a coin $n$ times? 

Now for the situation where we want to compute the probability of getting a certain number $x$ of heads after flipping the coin $n$ times, the appropriate probability mass function would be

$p(x) = \left(\begin{array}{c} n \\ x \end{array}\right)\left(\frac{1}{2}\right)^{x}\left(1 - \frac{1}{2}\right)^{n - x},$

where $x\in \{0,1,2,3,\ldots, n\}$, and $\left(\begin{array}{c} n \\ x \end{array}\right) = \frac{x!}{x!(n - x)!}$. 

**Note:** This is derived by simple counting.

Again, this function is implemented in R. For example, we can compute the probability of getting 5 heads out of 10 flips:
```{r}
dbinom(5,size=10,prob=0.5)
```
and also the probability of getting at most 5 heads out of ten flips:
```{r}
pbinom(5,size=10,prob=0.5)
```

Of course we can visualize the probability mass function for getting $x$ heads out of say 10 flips:
```{r}
gf_dist("binom",params = list(size=10,prob=0.5),kind="density")
```

While the distribution function is a little more complicated to write down, it is easy to visualize:
```{r}
gf_dist("binom",params = list(size=10,prob=0.5),kind="cdf")
```


**Exercise** Try changing the values for $n$ in the code that is used to create the previous figures. 


What we have developed here is a mathematical model, or *probability model* for modeling the random experiment of flipping a two-sided fair coin some set number of times. However, we still have not really addressed exactly what it is we mean by a **probability**. We just sort of asserted that the probability of getting heads after one flip of a (fair) coin is $\frac{1}{2}$.

What we can do is use our mathematical model to simulate the experiment of coin flipping in order to get some sense for what we might mean by a probability. That is, we can simulate coin flipping. This is done in R as follows:
```{r}
rbinom(10,size=1,prob=0.5)
```

Everytime we run this function we get a different number of 0's and 1's. 

Let's add up the number of 1's and divide by the number of flips:
```{r}
n <- 10
sum(rbinom(n,size=1,prob=0.5))/n
```
Notice that the values change each time we flip the coin. 

Observe what happens as we take $n$ increasingly larger:
```{r,echo=FALSE}
trials <- as.integer(10^(1:6))
probability <- numeric(length(trials))
for (i in 1:6){
  probability[i] <- sum(rbinom(trials[i],size=1,prob=0.5))/trials[i]
}
sim_df <- data.frame(trials=as.character(trials),probability=probability)
kable(sim_df)
```


As $n$ gets very large there is little variation of the result from 0.5. What this captures is the idea that in the long run (as $n\rightarrow \infty$) we expect that flipping a coin will produce as many heads as it will tails. 

# Comparison with the Real World

Now an actual experiment:  

Everyone flip a quarter 20 times and report the number of heads that you get. We can record the results. (Why is flipping 12 quarters 20 times each equivalent to flipping 1 quarter 240 times?) 
```{r}
n_trials <- 240
n_heads <- 122
n_tails <- (n_trials-n_heads)
df <- data.frame(flips=c(rep(0,n_tails),rep(1,n_heads)))
df %>% ggplot(aes(x=flips)) + geom_bar()
```

Let's compute the proportion of heads out of the total number of trials:
```{r}
(test_prop <- n_heads/n_trials)
```
**Question:** Are you surprised by this result? Why or why not? 

Note that the proportion $p_{\text{heads}} = \frac{n_{\text{heads}}}{n_{\text{trials}}}$ of heads out of $n_{\text{trials}}$ tosses of a coin is an example of a **statistic**. In general, a statistic is a function that takes the data as an input and returns a real number. Typically this number will provide some kind of useful summary of the data. The value `r test_prop` is what we call an **observed** or a **sample** value for our statistic. The reason for this is, if we were to repeat the experiment of flipping a coin `r n_trials` times, each time we conducted this expriment we would likely observe a different value for our statistic $p_{\text{heads}} = \frac{n_{\text{heads}}}{n_{\text{trials}}}$.     

**Question:** A typical question in probability and statistics is, what is the distribution for a statistic like $p_{\text{heads}} = \frac{n_{\text{heads}}}{n_{\text{trials}}}$ when the experiment is repeated a very large number of times? Such a distribution is called a sampling distribution for the statistic. We can perform a simulation to get  sense of what is the sampling distribution for the proportion $p_{\text{heads}} = \frac{n_{\text{heads}}}{n_{\text{trials}}}$. 

```{r}
N <- 5000
result <- do(N)*c(p_heads = sum(rbinom(n_trials,1,0.5))/n_trials)
result %>% gf_histogram(~p_heads)
```


Let's return to the data from our experiment. What is the probability of getting `r n_heads` number of heads out of `r n_trials` tosses of a coin (assuming that the coin is fair)? 
```{r}
dbinom(n_heads,size=240,prob=0.5)
```


Let's look at the mass function for the case of $n=240$:
```{r}
gf_dist("binom",params=list(size=n_trials,prob=0.5),kind="density")
```


Based on this, is the probability of getting $n_{\text{heads}}$ heads out of $n_{\text{trials}}$ flips high or low? The distribution function may also be useful to look at:
```{r}
gf_dist("binom",params=list(size=n_trials,prob=0.5),kind="cdf")
```


How come we do not get exaclty half heads and half tails in the real experiment? There are two possibilities:

1) Randomness
2) The coins are not really fair

How do we determine which of these two possibilities is the likely culprit? Well, if the coins are not fair, then the probability of getting heads (in a single flip) is 

$p(x) = P(X=x) = q^{x}\left(1 - q\right)^{1 - x}$

where the **parameter** $q$ is some value (between 0 and 1) that is not equal to $\frac{1}{2}$.

**Exercise** Think about why this might be. 

On the other hand, if the coins are fair then we expect that the probability of getting heads after one flip is $\frac{1}{2}$. So assume that the coin is fair and ask "Does the sample data that we have collected support this hypothesis or not?" This is where we transition from probability to (inferential) **statistics**. 

# From Probability to Statistics

We begin by forming a **hypothesis**, if the coins are fair then we expect that the probability of getting heads after one flip is $\frac{1}{2}$. Does the sample data that we have collected support this hypothesis or not? That is what we will address. Formally, we state a **null hypothesis**:

* $H_{0}$ - the coin is fair, *i.e.* $q = \frac{1}{2}$

to which there corresponds a mutually exclusive **alternative hypothesis**

* $H_{a}$ - the coin is not fair, *i.e.* $q \neq \frac{1}{2}$

What we have to do is to "test" the null-hypothesis. This is done as follows: 

1) Assume the null-hypothesis is true.
2) Compute the probability of getting a result that is at least as extreme as your data.
3) Decided on a cut-off probability.
4) If the probability of getting a restult that is at least as extreme as your data is less than the cut-off value, then you conclude that the data provides sufficient evidence to reject the null hypothesis. Otherwise, you do not reject the null hypothesis. 

Here we go:

Assuming that the coin(s) is (are) fair, what is the probability of getting a result that is at least as extreme as getting $n_{\text{heads}}$ heads out of $n_{\text{trials}}$ tosses? This can be calculated in R as follows: 
```{r}
(p <- 2*(1 - pbinom(n_heads-1,n_trials,0.5))) 
```

Note that we multiply by two because this is a symmetric distribution and we want to conduct a **two-sided** test. 


What this says is that under the null hypothesis ($q=\frac{1}{2}$) about `r round(p,2)` \% of the time we expect to get a result that is at least as extreme as getting $n_{\text{heads}}$ heads. While this probability is not high, it is also not extremely low. Thus, we fail to reject the null hypothesis. 

R will do all of this "hypothesis testing" for us automatically: 
```{r}
(b_test<-binom.test(n_heads,n=n_trials,p=0.5,alternative="two.sided"))
```

In addition to the probability value `r b_test$p.value` (*i.e.* the p-value), we also get a **confidence interval** (or at least an estimation for one) for the parameter $q$. In this case, we get a 95\% confidence interval. Again from Wikipedia:


> Were this procedure to be repeated on numerous samples, the fraction of calculated confidence intervals (which would differ for each sample) that encompass the true population parameter would tend toward 95\%. Thus,tThe (95\%) confidence interval represents values for the population parameter ($q$) for which the difference between the parameter and the observed estimate is not statistically significant at the 5\% level. A 95\% confidence interval does not mean that for a given realized interval there is a 95\% probability that the population parameter lies within the interval (*i.e.*, a 95\% probability that the interval covers the population parameter).


# Further Motivation

Consider the mpg data set cotained in the ggplot2 package:
```{r}
head(mpg)
```


This data contains information for various automobiles. Among the data are values for the class and highway mileage for each vehicle. Suppose we want to know if compact cars really get better highway driving gas mileage than midsize cars. First, we compute the **mean** and **standard deviation** (and also **standard error**) for the highway gas mileage per class:  

```{r}
mpg_stats <- mpg %>% 
  group_by(class) %>% 
  summarise(hwy_mean=mean(hwy,na.rm=T),hwy_sd=sd(hwy,na.rm=T),hwy_se=plotrix::std.error(hwy,na.rm=T))
```

Here is the result:
```{r}
mpg_stats
```

Is the difference in the mean highway gas mileage between compact and midsize cars significant? These leads to a hypothesis that we can test.

**Exercise** State a null hypothesis for this scenario. 


The following code carries out the hypothesis test. Later we will examine the details of this type of test. 

```{r}
class_1 <- "midsize"
class_2 <- "compact"
```

We first look at the mean highway gas mileage in each class and also include the standard error:
```{r}
mpg_stats %>% filter(class %in% c(class_1,class_2)) %>%
  ggplot(aes(x=class,y=hwy_mean)) + 
  geom_bar(stat = "identity",width=0.6) + 
  geom_errorbar(mapping=aes(ymin=hwy_mean-hwy_se,ymax=hwy_mean+hwy_se),width=0.2)
```

Is this difference significant? Let's see the test:

```{r}
df_testing <- mpg %>% filter(class %in% c(class_1,class_2)) %>%
  select(class,hwy)
with(df_testing,t.test(hwy~class,var.equal=FALSE,alternative="two.sided",mu=0))
```

Based on the results of the test, the data does not provide enough justification for rejecting the null hypothesis that there is no difference in the population mean for highway gas mileage between compact and midsize vehicles. 

What we need to understand first in order to grasp the details of the previous test are answers to the following questions:

1) What are the relevant **parameters** for stating the null hypothesis "numerically"? 
2) What is an appropriate **random variable(s)** and **probability distribution** to use in order to compute the probability of getting a result that is at least as extreme as the data under the null hypothesis? That is, what is the relevant probability model and how do we use it to compute the $p$-value?


In order to answer these questions and to go even further in order to carry out hypothesis tests for the broadest possible range of applications, it is worth some effort to understand probability models in a general abstract mathematical way. This will save us time in the long run since then statistics will not be "cookbook" but very natural. Thus, theory becomes very practical. Motivated as such, we now begin the course in earnest.   

